{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Feature Stencil Code\n",
    "# Written by James Hays for CS 143 @ Brown / CS 4476/6476 @ Georgia Tech with Henry Hu <henryhu@gatech.edu>\n",
    "# Edited by James Tompkin\n",
    "# Adapted for python by asabel and jdemari1 (2019)\n",
    "\n",
    "import csv\n",
    "import sys\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage import io, filters, feature, img_as_float32\n",
    "from skimage.transform import rescale\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from utils import *\n",
    "import cv2\n",
    "import student\n",
    "import visualize\n",
    "from helpers import cheat_interest_points, evaluate_correspondence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script\n",
    "# (1) Loads and resizes images\n",
    "# (2) Finds interest points in those images                 (you code this)\n",
    "# (3) Describes each interest point with a local feature    (you code this)\n",
    "# (4) Finds matching features                               (you code this)\n",
    "# (5) Visualizes the matches\n",
    "# (6) Evaluates the matches based on ground truth correspondences\n",
    "\n",
    "def load_data(file_name):\n",
    "    \"\"\"\n",
    "    1) Load stuff\n",
    "    There are numerous other image sets in the supplementary data on the\n",
    "    project web page. You can simply download images off the Internet, as\n",
    "    well. However, the evaluation function at the bottom of this script will\n",
    "    only work for three particular image pairs (unless you add ground truth\n",
    "    annotations for other image pairs). It is suggested that you only work\n",
    "    with the two Notre Dame images until you are satisfied with your\n",
    "    implementation and ready to test on additional images. A single scale\n",
    "    pipeline works fine for these two images (and will give you full credit\n",
    "    for this project), but you will need local features at multiple scales to\n",
    "    handle harder cases.\n",
    "\n",
    "    If you want to add new images to test, create a new elif of the same format as those\n",
    "    for notre_dame, mt_rushmore, etc. You do not need to set the eval_file variable unless\n",
    "    you hand create a ground truth annotations. To run with your new images use\n",
    "    python main.py -p <your file name>.\n",
    "\n",
    "    :param file_name: string for which image pair to compute correspondence for\n",
    "\n",
    "        The first three strings can be used as shortcuts to the\n",
    "        data files we give you\n",
    "\n",
    "        1. notre_dame\n",
    "        2. mt_rushmore\n",
    "        3. e_gaudi\n",
    "\n",
    "    :return: a tuple of the format (image1, image2, eval file)\n",
    "    \"\"\"\n",
    "\n",
    "    # Note: these files default to notre dame, unless otherwise specified\n",
    "    # image1_file = \"../data/NotreDame/NotreDame1.jpg\"\n",
    "    # image2_file = \"../data/NotreDame/NotreDame2.jpg\"\n",
    "\n",
    "    image1_file = \"../data/NotreDameCopy/Notre1.jpg\"\n",
    "    image2_file = \"../data/NotreDameCopy/Notre2.jpg\"\n",
    "\n",
    "    eval_file = \"../data/NotreDame/NotreDameEval.mat\"\n",
    "\n",
    "    if file_name == \"notre_dame\":\n",
    "        pass\n",
    "    elif file_name == \"mt_rushmore\":\n",
    "        image1_file = \"../data/MountRushmore/Mount_Rushmore1.jpg\"\n",
    "        image2_file = \"../data/MountRushmore/Mount_Rushmore2.jpg\"\n",
    "        eval_file = \"../data/MountRushmore/MountRushmoreEval.mat\"\n",
    "    elif file_name == \"e_gaudi\":\n",
    "        image1_file = \"../data/EpiscopalGaudi/EGaudi_1.jpg\"\n",
    "        image2_file = \"../data/EpiscopalGaudi/EGaudi_2.jpg\"\n",
    "        eval_file = \"../data/EpiscopalGaudi/EGaudiEval.mat\"\n",
    "\n",
    "    image1 = cv2.imread(image1_file)\n",
    "    image1 = image1.astype(np.float32)/255\n",
    "    image1 = image1[:, :, ::-1]\n",
    "    # image2 = img_as_float32(io.imread(image2_file))\n",
    "    image2 = cv2.imread(image2_file)\n",
    "    image2 = image2.astype(np.float32)/255\n",
    "    image2 = image2[:, :, ::-1]\n",
    "\n",
    "    return image1, image2, eval_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) Load in the data\n",
    "image1, image2, eval_file = load_data(\"notre_dame\")\n",
    "# # width and height of each local feature, in pixels\n",
    "feature_width = 16\n",
    "scale_factor = 0.5\n",
    "\n",
    "image1 = cv2.resize(image1, (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "image2 = cv2.resize(image2, (0, 0), fx=scale_factor, fy=scale_factor)\n",
    "image1_bw = cv2.cvtColor(image1, cv2.COLOR_RGB2GRAY)\n",
    "image2_bw = cv2.cvtColor(image2, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    }
   ],
   "source": [
    "image_file = \"../data/NotreDameCopy/Notre1.jpg\"\n",
    "image_test=cv2.imread(image_file)\n",
    "Xderivative = cv2.Sobel(image_test, cv2.CV_64F,1,0,ksize=5)\n",
    "Yderivative = cv2.Sobel(image_test, cv2.CV_64F,0,1,ksize=5)\n",
    "plt.imshow(Yderivative)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Getting interest points...\nGetting interest points of image1...\nGetting interest points of image2...\n3025 corners in image 1, 3025 corners in image 2\nDone!\n"
    }
   ],
   "source": [
    "# (2) Find distinctive points in each image. See Szeliski 4.1.1\n",
    "# !!! You will need to implement get_interest_points. !!!\n",
    "\n",
    "print(\"Getting interest points...\")\n",
    "\n",
    "# For development and debugging get_features and match_features, you will likely\n",
    "# want to use the ta ground truth points, you can comment out the precedeing two\n",
    "# lines and uncomment the following line to do this.\n",
    "\n",
    "#(x1, y1, x2, y2) = cheat_interest_points(eval_file, scale_factor)\n",
    "print(\"Getting interest points of image1...\")\n",
    "(x1, y1) = student.get_interest_points(image1_bw, feature_width)\n",
    "print(\"Getting interest points of image2...\")\n",
    "(x2, y2) = student.get_interest_points(image2_bw, feature_width)\n",
    "\n",
    "# if you want to view your corners uncomment these next lines!\n",
    "# print(\"Show the interest points!\")\n",
    "# plt.imshow(image1, cmap=\"gray\")\n",
    "# plt.scatter(x1, y1, alpha=0.9, s=3)\n",
    "# plt.show()\n",
    "\n",
    "# plt.imshow(image2, cmap=\"gray\")\n",
    "# plt.scatter(x2, y2, alpha=0.9, s=3)\n",
    "# plt.show()\n",
    "\n",
    "print('{:d} corners in image 1, {:d} corners in image 2'.format(len(x1), len(x2)))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Getting features...\nimage1_features len: 3025\nDone!\n"
    }
   ],
   "source": [
    "# 3) Create feature vectors at each interest point. Szeliski 4.1.2\n",
    "# !!! You will need to implement get_features. !!!\n",
    "\n",
    "print(\"Getting features...\")\n",
    "image1_features = student.get_features(image1_bw, x1, y1, feature_width)\n",
    "image2_features = student.get_features(image2_bw, x2, y2, feature_width)\n",
    "print(\"image1_features len:\",len(image1_features))\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Matching features...\n164 matches from 3025 corners\n"
    }
   ],
   "source": [
    "# 4) Match features. Szeliski 4.1.3\n",
    "# !!! You will need to implement match_features !!!\n",
    "\n",
    "print(\"Matching features...\")\n",
    "matches, confidences = student.match_features(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(x1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Done!\nmatche shape: (164, 2)\n"
    }
   ],
   "source": [
    "if len(matches.shape) == 1:\n",
    "    print( \"No matches!\")\n",
    "\n",
    "print(\"Done!\")\n",
    "print(\"matche shape:\",matches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: 164\n",
      "105.0 total good matches, 44 total bad matches.\n",
      "70.46979865771812% precision\n",
      "68% accuracy (top 100)\n",
      "Vizualizing...\n"
     ]
    }
   ],
   "source": [
    "# 5) Visualization\n",
    "\n",
    "# You might want to do some preprocessing of your interest points and matches\n",
    "# before visualizing (e.g. only visualizing 100 interest points). Once you\n",
    "# start detecting hundreds of interest points, the visualization can become\n",
    "# crowded. You may also want to threshold based on confidence\n",
    "\n",
    "# visualize.show_correspondences produces a figure that shows your matches\n",
    "# overlayed on the image pairs. evaluate_correspondence computes some statistics\n",
    "# about the quality of your matches, then shows the same figure. If you want to\n",
    "# just see the figure, you can uncomment the function call to visualize.show_correspondences\n",
    "\n",
    "\n",
    "num_pts_to_visualize = matches.shape[0]\n",
    "print(\"Matches: \" + str(num_pts_to_visualize))\n",
    "# visualize.show_correspondences(image1, image2, x1, y1, x2, y2, matches, filename=args.pair + \"_matches.jpg\")\n",
    "\n",
    "## 6) Evaluation\n",
    "# This evaluation function will only work for the Notre Dame, Episcopal\n",
    "# Gaudi, and Mount Rushmore image pairs. Comment out this function if you\n",
    "# are not testing on those image pairs. Only those pairs have ground truth\n",
    "# available.\n",
    "#\n",
    "# It also only evaluates your top 100 matches by the confidences\n",
    "# that you provide.\n",
    "#\n",
    "# Within evaluate_correspondences(), we sort your matches in descending order\n",
    "#\n",
    "num_pts_to_evaluate = matches.shape[0]\n",
    "\n",
    "evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "    x1, y1, x2, y2, matches, confidences, num_pts_to_visualize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3025, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}